# Query Classification: Hybrid Approach

## Overview

The chatbot uses a **hybrid approach** for query classification that combines:
1. **Rule-based classification** (fast, predictable, no LLM cost)
2. **LLM-based classification** (intelligent, catches edge cases)

## Classification Flow

```
User Query
    ‚Üì
1. Rule-Based Checks (Fast)
    ‚îú‚îÄ Greeting? ‚Üí Handle greeting
    ‚îú‚îÄ Coverage query? ‚Üí Show fund list
    ‚îú‚îÄ Advice query? ‚Üí Refuse politely
    ‚îî‚îÄ Out-of-context? ‚Üí Refuse politely
    ‚Üì
2. LLM Classification (If enabled & no match above)
    ‚îú‚îÄ Ask LLM to classify intent
    ‚îú‚îÄ Validates classification
    ‚îî‚îÄ Returns classification or falls back
    ‚Üì
3. Default to Factual Query
    ‚îî‚îÄ Process with RAG
```

## Why Hybrid Approach?

### Rule-Based Advantages ‚úÖ
- **Fast**: No LLM call needed
- **Predictable**: Same input ‚Üí same output
- **Cost-effective**: No API costs
- **Reliable**: Works even if LLM is down

### LLM-Based Advantages ‚úÖ
- **Intelligent**: Understands nuanced intent
- **Flexible**: Adapts to different phrasings
- **Catches edge cases**: Better than regex patterns
- **Natural language understanding**: "Is HDFC good?" ‚Üí advice

### Combined Benefits üéØ
- Rules catch 90% of cases instantly (no cost)
- LLM catches the remaining 10% edge cases
- Graceful fallback if LLM fails

## Query Types

### 1. Greeting
**Examples:**
- "hi"
- "hello"
- "good morning"

**Detection:** Rule-based (keyword matching)
**Response:** Welcome message with capabilities

---

### 2. Coverage
**Examples:**
- "what schemes do you have"
- "what can i ask you"
- "list all funds"

**Detection:** Rule-based (regex patterns) + LLM backup
**Response:** List of 4 HDFC funds + example questions

---

### 3. Factual
**Examples:**
- "What is the expense ratio of HDFC Large Cap Fund?"
- "ELSS lock-in period?"
- "How to download CAS?"

**Detection:** LLM classification (if enabled)
**Response:** RAG-based answer with source citation

---

### 4. Advice
**Examples:**
- "Should I invest in HDFC Large Cap?"
- "Which fund is better?"
- "Is ELSS good for me?"

**Detection:** Rule-based (keywords) + LLM backup
**Response:** Polite refusal + AMFI link

---

### 5. Out-of-Context
**Examples:**
- "Who is PM of India?"
- "Tell me a joke"
- "565665" (random numbers)

**Detection:** Rule-based (patterns) + LLM backup
**Response:** Polite refusal + redirect to MF topics

---

## Configuration

### Enable/Disable LLM Classification

**In `.env` file:**
```bash
# Enable LLM classification for better intent understanding
USE_LLM_CLASSIFICATION=true

# Disable to save costs (use only rule-based)
USE_LLM_CLASSIFICATION=false
```

**Default:** `true` (enabled)

### When to Disable?

Disable LLM classification if:
- You want to minimize LLM API costs
- Response time is critical (save ~200-500ms)
- Rule-based classification is sufficient for your use case
- Running in resource-constrained environment

### When to Enable?

Enable LLM classification if:
- You want best accuracy (recommended for production)
- Users ask questions in non-standard ways
- Edge cases are important to catch
- Cost is not a major concern

## Cost Impact

**With LLM Classification:**
- Extra 1 LLM call per query (only for ambiguous cases)
- ~20 tokens per classification call
- Minimal cost: ~$0.00001 per query (Gemini)

**Without LLM Classification:**
- No extra LLM calls
- Relies purely on regex/keyword patterns
- Faster but may miss edge cases

## Examples

### Example 1: Clear Greeting (Rule-Based)
```
Query: "hello"
Rule Check: ‚úÖ Matches greeting pattern
LLM Called: ‚ùå No (already classified)
Classification: greeting
Time: <10ms
```

### Example 2: Ambiguous Query (LLM-Based)
```
Query: "Is the large cap scheme any good?"
Rule Check: ‚ùì Could be advice or factual
LLM Called: ‚úÖ Yes
LLM Output: "advice"
Classification: advice
Time: ~500ms
```

### Example 3: Clear Factual (Rule-Based Skip)
```
Query: "What is the expense ratio of HDFC Large Cap Fund?"
Rule Check: ‚ùå No pattern match (not greeting/advice/out-of-context)
LLM Called: ‚úÖ Yes (if enabled)
LLM Output: "factual"
Classification: factual
Time: ~500ms (then RAG processing)
```

### Example 4: Random Numbers (Rule-Based)
```
Query: "565665"
Rule Check: ‚úÖ Matches out-of-context (only digits)
LLM Called: ‚ùå No (already classified)
Classification: out_of_context
Time: <10ms
```

## Implementation Details

### LLM Classification Prompt

```python
"""Classify this user query into ONE category:

Query: "{query}"

Categories:
1. greeting - User is saying hi, hello, or greeting
2. coverage - User asking what funds/schemes/questions you cover
3. factual - User wants factual information about mutual funds
4. advice - User seeking investment advice or recommendations
5. out_of_context - Completely unrelated to mutual funds

Return ONLY the category name (one word), nothing else.

Category:"""
```

### Validation

The LLM response is validated:
- Must be one of: `greeting`, `coverage`, `factual`, `advice`, `out_of_context`
- If invalid ‚Üí fallback to `factual`
- If LLM call fails ‚Üí fallback to rule-based classification

### Graceful Degradation

The system gracefully handles failures:
1. LLM classification throws error ‚Üí Use rule-based
2. Rule-based uncertain + LLM disabled ‚Üí Default to factual
3. Both fail ‚Üí Default to factual (safe choice)

## Testing LLM Classification

### Test Cases for LLM

These queries benefit most from LLM classification:

```python
# Nuanced advice queries
"Is large cap good?"
"Should I go with HDFC?"
"Which one would you recommend?"

# Conversational factual queries
"Tell me about the expense ratio"
"I want to know about ELSS"
"Can you explain the lock-in period?"

# Ambiguous queries
"What do you think about large cap?"
"HDFC Flexi Cap - any thoughts?"
```

### Test Rule-Based Only

To test with rule-based only:

```bash
# Set environment variable
export USE_LLM_CLASSIFICATION=false

# Run tests
python quick_health_check.py
```

## Performance Comparison

| Metric | Rule-Based Only | Hybrid (Rule + LLM) |
|--------|----------------|---------------------|
| **Speed** | <10ms | 10-500ms |
| **Cost** | $0 | ~$0.00001/query |
| **Accuracy** | ~90% | ~98% |
| **Edge Cases** | Misses some | Catches most |
| **Reliability** | Very high | High |

## Recommendations

### For Development
- ‚úÖ Enable LLM classification
- Test with diverse queries
- Monitor edge cases

### For Production
- ‚úÖ Enable LLM classification (recommended)
- Monitor costs if high traffic
- Consider disabling if budget-constrained

### For High-Traffic Apps
- Consider caching classification results
- Use rule-based for common queries
- LLM only for first-time queries

## Monitoring

### What to Monitor
1. **Classification accuracy**: Are queries classified correctly?
2. **LLM classification rate**: How often is LLM called?
3. **Response time**: Impact of LLM classification
4. **Cost**: LLM API usage for classification

### Debug Logging

Enable debug logging to see classification decisions:

```python
print(f"Query: {query}")
print(f"Rule-based: {rule_classification}")
print(f"LLM-based: {llm_classification}")
print(f"Final: {final_classification}")
```

## Future Improvements

1. **Caching**: Cache classification results for common queries
2. **Fine-tuning**: Train a small classifier model
3. **Confidence scores**: Have LLM return confidence
4. **A/B Testing**: Compare rule-based vs LLM classification
5. **Analytics**: Track which queries benefit from LLM

## Summary

The hybrid approach gives you:
- ‚úÖ **Best of both worlds**: Fast rules + intelligent LLM
- ‚úÖ **Cost-effective**: Only uses LLM when needed
- ‚úÖ **Reliable**: Graceful fallback
- ‚úÖ **Configurable**: Can disable LLM classification
- ‚úÖ **Production-ready**: Handles edge cases

**Recommendation:** Keep LLM classification **enabled** for best user experience.

